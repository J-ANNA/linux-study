磁盘配额(Quota)与进阶文件系统管理

Quota针对整个filesystem的限制项目主要分为以下几个部分：
1. 容量限制或文档数量限制(block 或 inode)
2. 柔性劝导与硬件规定(soft / hard)
3. 会倒数计时的宽限时间(grace time)


范例：Quota 的设定流程

制作账号环境
[root@www ~]# vi addaccount.sh
#!/bin/bash
#使用script来建立实验 quota 所需的环境
groupadd myquotagrp
for username in myquota1 myquota2 myquota3 myquota4 myquota5
do
	useradd -g myquotagrp $username
	echo "password" | passwd --stdin $username
done

[root@www ~]#sh addaccount.sh

流程一：文件系统支援
1.查看 /home 是否是个独立的 filesystem
[root@www ~]# df -h /home
Filesystem Size Used Avail Use% Mounted on
/dev/hda3 4.8G 740M 3.8G 17% /home <==该主机的 /home 确实是独立的
[root@www ~]# mount | grep home
/dev/hda3 on /home type ext3 (rw)

2.手动加入quota的支持
[root@www ~]#mount -o remount,usrquota,grpquota /home
[root@www ~]#mount | grep home
/dev/hda3 on /home type ext3 (rw,usrquota,grpquota)
# 重点就在于 usrquota, grpquota ！注意写法！

3.不过手动挂载的数据在下次重新挂载就会消失，因此最好写入配置文件当中
[root@www ~]# vi /etc/fstab
LABEL=/home /home ext3 defaults,usrquota,grpquota 1 2
# 其他顷目并没有列出来。重点在于第四字段。于 default 后面加上两个参数

[root@www ~]# umount /home
[root@www ~]# mount -a
[root@www ~]# mount | grep home
/dev/hda3 on /home type ext3 (rw,usrquota,grpquota)

(修改完 /etc/fstab 后，务必要测试一下。若有发生错误得要赶紧处理。因为这个文档如果修改错误，会造成无法开机完全的情况)

流程二：建立quota记录文件
quotacheck: 扫描文件系统并建立 Quota 的记录文件
quotacheck [-avugfM] [/mount_point]
选项与参数：
-a: 扫描所有在 /etc/mtab内，含有quota支持的filesystem,加上次参数后，/mount_point可不必写，因为扫描所有的filesystem
-u: 针对用户扫描文档与目录的使用情况，建立 aquota.user
-g: 针对群组扫描文档与目录的使用情况，建立 aquota.group
-v: 显示扫描过程的信息
-f: 强制扫描文件系统，并写入新的 quota 配置文件(危险)
-M: 强制以读写的方式扫描文件系统，只有在特殊情况下才会使用

# 针对整个系统含有 usrquota, grpquota 参数的文件系统进行 quotacheck 扫描
[root@www ~]# quotacheck -avug
quotacheck: Scanning /dev/hda3 [/home] quotacheck: Cannot stat old user quota
file: No such file or directory <==有找到文件系统，但尚未制作记录文件！
quotacheck: Cannot stat old group quota file: No such file or directory
quotacheck: Cannot stat old user quota file: No such file or directory
quotacheck: Cannot stat old group quota file: No such file or directory done <==上面三个错误只是说明记录文件尚未建立而已，可以忽略不理！
quotacheck: Checked 130 directories and 107 files <==实际搜寻结果
quotacheck: Old file not found.
quotacheck: Old file not found.
# 若执行这个指令却出现如下的错误讯息，表示你没有任何文件系统有启动 quota 支持！
# quotacheck: Can't find filesystem to check or filesystem not mounted with quota option.

[root@www ~]# ll -d /home/a*
-rw------- 1 root root 8192 Mar 6 11:58 /home/aquota.group
-rw------- 1 root root 9216 Mar 6 11:58 /home/aquota.user
# 在该案例中，/home 是独立的文件系统，因此搜寻结果会将两个记录文件放
在 /home 底下。这两个文档就是 Quota 最重要的信息了！

# 如果因为特殊需求需要强制扫描已挂载的文件系统时
[root@www ~]# quotacheck -avug -mf
quotacheck: Scanning /dev/hda3 [/home] done
quotacheck: Checked 130 directories and 109 files
# 资料要简洁很多。因为有记录文件存在嘛。所以警告讯息不会出现

注意：
1. 不用手动编辑记录文件，因为那两个文件时quota自动数据文件，并不是纯文本文档，且该文档会一直变动。
2. 要建立 aquota.user, aquota.group,使用 quotacheck 指令，不是手动编辑。


流程三：Quota启动、关闭与限制值设定
quotaon: 启动 quota 的服务
quotaon [-avug]
quotaon [-vug] [/mount_point]
选项与参数：
-u: 针对使用者启动 quota (aquota.user)
-g: 针对群组启动 quota (aquota.group)
-v: 显示启动过程的相关讯息
-a: 根据 /etc/mtab 内的 filesystem 设定启动有关的 quota, 若不加 -a 的话，则后面就需要加上特定的那个 filesystem 

# 由于我们要启动 user/group 的 quota ，所以使用底下的语法即可
[root@www ~]# quotaon -auvg
/dev/hda3 [/home]: group quotas turned on
/dev/hda3 [/home]: user quotas turned on

# 特殊用法，假如你的启动 /var 的 quota 支持，那么仅启动 user quota 时
[root@www ~]# quotaon -uv /var

(这个 "quotaon -auvg" 的指令几乎只在第一次启动 quota 时才需要进行。因为下次等你重新启动系统时，系统的 /etc/rc.d/rc.sysinit 这个初始化脚本就会自动的下达这个指令了)

quotaoff: 关闭 quota 的服务
quotaoff [-a]
quotaoff [-ug] [/mount_point]
选项与参数：
-a: 全部的filesystem的quota都关闭 (根据 /etc/mtab)
-u: 仅针对后面接的那个 /mount_point 关闭 user quota
-g: 仅针对后面接的那个 /mount_point 关闭 group quota

edquota: 编辑账号/群组的限值与宽限时间
edquota [-u username] [-g groupname]
edquota -t 修改宽限时间
edquota -p 范本账号 -u 新账号
选项与参数：
-u: 后面接账号名称。可以进入 quota 的编辑画面(vi)去设定 username 的限制值
-g: 后面接组名称。可以进入 quota 的编辑画面(vi)去设定 groupname 的限制值
-t: 可以修改宽限时间
-p: 复制范本。那个模板账号为已经存在并且已设定好 quota 的使用者，意义为 "将范本账号这个人的 quota 限制值复制给新账户"

设定 myquota1 这个使用者的 quota 限制值
[root@www ~]# edquota -u myquota1
Disk quotas for user myquota1 (uid 710):
Filesystem blocks soft hard inodes soft hard
/dev/hda3     80    0   0     10     0    0
设置
Disk quotas for user myquota1 (uid 710):
Filesystem blocks   soft   hard   inodes soft hard
/dev/hda3   80     250000 300000    10    0     0

# 将 myquota1 的限制值复制给其他四个账号
[root@www ~]# edquota -p myquota1 -u myquota2
[root@www ~]# edquota -p myquota1 -u myquota3
[root@www ~]# edquota -p myquota1 -u myquota4
[root@www ~]# edquota -p myquota1 -u myquota5

更改群组的quota限额
[root@www ~]# edquota -g myquotagrp
Disk quotas for group myquotagrp (gid 713):
Filesystem blocks   soft    hard  inodes soft hard
/dev/hda3    400   900000 1000000   50     0   0
# 记得，单位为 KB 

设置宽限时间
# 宽限时间原本为 7 天，将他改成 14 天吧！
[root@www ~]# edquota -t
Grace period before enforcing soft limits for users:
Time units may be: days, hours, minutes, or seconds
Filesystem    Block grace period      Inode grace period
/dev/hda3           14days                  7days
# 原本是 7days ，我们将他给改为 14days


流程四：Quota 限制值的报表
quota: 单一用户的 quota 报表
quota [-uvs] [username]
quota [-gvs] [groupname]
选项与参数：
-u: 后面可以接 username,表示显示出该用户的quota限制值。若不接 username,表示显示出执行者的 quota 限制值 
-g: 后面接 groupname, 表示显示出该群组的 quota 限制值
-v: 显示每个用户在 filesystem 的 quota 值
-s: 使用1024为倍数来指定单位，会显示如 M 之类的单位

# 直接使用 quota 去显示出 myquota1 与 myquota2 的限额
[root@www ~]# quota -uvs myquota1 myquota2
Disk quotas for user myquota1 (uid 710):
Filesystem blocks quota limit grace files quota limit grace
/dev/hda3    80    245M  293M         10    0     0
Disk quotas for user myquota2 (uid 711):
Filesystem blocks quota limit grace files quota limit grace
/dev/hda3     80   245M 293M          10    0     0
# 这个指令显示出来的数据跟 edquota 几乎是一模一样的。只是多了个 grace项目。
# 你会发现 grace 底下没有任何数据，这是因为我们的使用量 (80) 尚未超过soft

# 显示出 myquotagrp 的群组限额
[root@www ~]# quota -gvs myquotagrp
Disk quotas for group myquotagrp (gid 713):
Filesystem blocks quota limit grace files quota limit grace
/dev/hda3    400  879M  977M         50     0     0

repquota: 针对文件系统的限额做报表
repquota -a [-vugs]
选项与参数：
-a: 直接到 /etc/mtab 搜寻具有 quota 标志的 filesystem,并报告 quota 的结果
-v: 输出的数据将含有 filesystem 相关的细部信息
-u: 显示出用户的 quota 限值(这是默认值)
-g: 显示出个别群组的 quota 限值
-s: 使用M,G为单位显示结果

# 查询本案例中所有使用者的 quota 限制情况：
[root@www ~]# repquota -auvs
*** Report for user quotas on device /dev/hda3 <==针对 /dev/hda3
Block grace time: 14days; Inode grace time: 7days <==block 宽限时间为14 天
		   Block limits 			   File limits
User      used  soft  hard grace   used soft hard grace
----------------------------------------------------------------------
root --   651M   0     0             5   0    0
myquota1 -- 80  245M  293M           10  0    0
myquota2 -- 80  245M  293M           10  0    0
myquota3 -- 80  245M  293M           10  0    0
myquota4 -- 80  245M  293M           10  0    0
myquota5 -- 80  245M  293M           10  0    0
Statistics: <==这是所谓的系统相关信息，用 -v 才会显示
Total blocks: 9
Data blocks: 2
Entries: 22
Used average: 11.000000


流程5：测试与管理
# 测试1：利用 myquota1 的身份，建置一个 270MB 的大档案，幵观察 quota结果
[myquota1@www ~]# dd if=/dev/zero of=bigfile bs=1M count=270
hda3: warning, user block quota exceeded.
270+0 records in
270+0 records out
283115520 bytes (283 MB) copied, 3.20282 seconds, 88.4 MB/s

(注意：使用 myquota1 的账号去进行 dd 指令)


warnquota: 对超过限额者发出警告信(email)
[root@www ~]# warnquota
执行 warnquota 可能也不会产生任何讯息以及信件，因为只有当使用者的 quota 有超过 soft 时，warnquota 才会发送警告信。警告信的内容编辑在 /etc/warnquota.conf 中。
[root@www ~]# vi /etc/warnquota.conf
# 先找到底下这几行的设定值：
SUBJECT = NOTE: You are exceeding your allocated disk space limits <==第 10 行
CC_TO = "root@localhost" <==第 11 行
MESSAGE = Your disk usage has exceeded the agreed limits on this server|Please delete any unnecessary files on following filesystems:|   <== 第 21 行
SIGNATURE = root@localhost <==第 25 行

让系统自动执行warnquota
[root@www ~]# vi /etc/cron.daily/warnquota
/usr/sbin/warnquota
# 只要这一行，且将执行文件以绝对路径的方式写入即可


setquota: 直接于指令中设定 quota 限额
setquota [-u|-g] 名称 block(soft) block(hard) inode(soft) inode(hard) 文件系统 
范例： 
[root@www ~]# setquota -u myquota5 100000 200000 0 0 /home


不更动既有系统的quota实例：使用 ln 实现
1. 将 /var/spool/mail 这个目录完整的移动到 /home 底下；
2. 利用 ln -s /home/mail /var/spool/mail 建立链接数据；
3. 将 /home 进行 quota 限额设定


软件磁盘阵列(Software RAID)

RAID(Redundant Arrays of Inexpensive Disks,磁盘阵列)：
RAID 可以透过一个技术(软件或硬件)，将多个较小的磁盘整合成为一个较大的磁盘装置；而这个较大的磁盘功能可不止是储存而已，他还具有数据保护的功能。整个 RAID 由亍选择的等级(level)不同，而使得整合后的磁盘具有不同的功能。
常见的level类型：
1. RAID-0(等量模式,stripe): 效能最佳
2. RAID-1(映像模式,mirror): 完整备份
3. RAID 0+1, RAID 1+0
4. RAID 5：效能与数据备份的均衡考虑
5. Spare Disk: 预备磁盘的功能

磁盘阵列的优点：
1. 数据的安全与可靠性
2. 读写效能
3. 容量


软件磁盘阵列的设定
mdadm --detail /dev/md0
mdadm --create --auto=yes /dev/md[0-9] --raid-devices=N --level=[015] --spare-devices=N /dev/sdx /dev/hdx...
选项与参数：
--create: 为建立 RAID 的选项
--auto=yes: 决定建立后面接的软件磁盘阵列装置，亦即 /dev/md0,/dev/md1...
--raid-devices=N: 使用几个磁盘(parition)作为磁盘阵列的装置
--spare-devices=N: 使用几个磁盘作为备用(spare)装置
--level=[015]: 设定这组磁盘阵列的等级。支持很多，不过建议只要用0,1,5即可
--detail: 后面所接的那个磁盘阵列装置的详细信息

查看软件磁盘阵列情况：
[root@www ~]# cat /proc/mdstat

格式化与挂载使用RAID
[root@www ~]# mkfs -t ext3 /dev/md0  #/dev/md0 作为装置被格式化
[root@www ~]# mkdir /mnt/raid
[root@www ~]# mount /dev/md0 /mnt/raid
[root@www ~]# df 

仿真RAID错误的救援模式
[root@www ~]# mdadm --manage /dev/md[0-9] [-add 装置] [--remove 装置] [--fail 装置]
选项与参数：
--add: 会将后面的装置加入到这个 md 中
--remove: 会将后面的装置由这个 md 中移除
--fail: 会将后面的装置设定成为出错的状态

# 1.假设 /dev/hda8 这个装置出错了。实际模拟的方式：
[root@www ~]# mdadm --manage /dev/md0 --fail /dev/hda8
mdadm: set /dev/hda8 faulty in /dev/md0
[root@www ~]# mdadm --detail /dev/md0

将出错的磁盘移除并加入新磁盘
# 建立新的分割槽
[root@www ~]# fdisk /dev/hda
Command (m for help): n
First cylinder (2668-5005, default 2668): <==这里按 [enter]
Using default value 2668
Last cylinder or +size or +sizeM or +sizeK (2668-5005, default 5005):
+1000M
Command (m for help): w
[root@www ~]# partprobe
# 此时系统会多一个 /dev/hda11 的分割槽

# 加入新的拔除有问题的磁盘
[root@www ~]# mdadm --manage /dev/md0 --add /dev/hda11 --remove
/dev/hda8
mdadm --detail /dev/md0


开启自动启动 RAID 并自动挂载
[root@www ~]# mdadm --detail /dev/md0 | grep -i uuid
UUID : 7c60c049:57d60814:bd9a77f1:57e49c5b
# 后面那一串数据，就是这个装置向系统注册的 UUID 标识符

# 开始设定 mdadm.conf
[root@www ~]# vi /etc/mdadm.conf
ARRAY /dev/md0 UUID=7c60c049:57d60814:bd9a77f1:57e49c5b
#     RAID装置    标识符内容
# 开始设定开机自动挂载并测试
[root@www ~]# vi /etc/fstab
/dev/md0 /mnt/raid ext3 defaults 1 2
[root@www ~]# umount /dev/md0; mount -a
[root@www ~]# df /mnt/raid
Filesystem 1K-blocks Used Available Use% Mounted on
/dev/md0 2916920 188464 2580280 7% /mnt/raid
# 你得确定可以顺利挂载，并且没有发生任何错误
(如果到这里都没有出现任何问题，接下就 reboot 你的系统并等待看看能否顺利的启动)

关闭软件RAID
# 1. 先卸除且删除配置文件内与这个 /dev/md0 有关的设定：
[root@www ~]# umount /dev/md0
[root@www ~]# vi /etc/fstab
#/dev/md0 /mnt/raid ext3 defaults 1 2
# 将这一行删除掉，或者是批注掉也可以

# 2. 直接关闭 /dev/md0 的方法
[root@www ~]# mdadm --stop /dev/md0
mdadm: stopped /dev/md0 <==这样就关闭了
[root@www ~]# cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
unused devices: <none> <==确实不存在任何数组装置
[root@www ~]# vi /etc/mdadm.conf
#ARRAY /dev/md0 UUID=7c60c049:57d60814:bd9a77f1:57e49c5b
# 一样，初除他或是批注他


逻辑滚动条(Logical Volume Manager)
可以弹性的调整 filesystem 的容量。LVM可以整合多个实体partition在一起，让这些partitions看起来就像是一个磁盘一样。而且，还可以在将来新增或移除其它的实体partition 到这个 LVM 管理的磁盘当中。

PV(Physical Volume,实体滚动条)
VG(Volume Group,滚动条群组)
PE(physical Extend,实体延伸区块)，是LVM的最小储存区块
LV(Logical Volume,逻辑滚动条)

LV的装置文件名通常指定为 "/dev/vgname/lvname"
LV：写入硬盘的两种方式
1. 线性模式(linear)
2. 交错模式(triped)

LVM实作流程
(LVM必需要核心有支持且需要安装lvm2这个软件。)
1. 先分割出4个partition,每个partition的容量均为1.5G左右，且system ID 需要为 8e;
2. 全部的partition整合成为一个VG,VG的名称设定为vbirdvg;且PE的大小为16MB;
3. 全部的VG容量都丢给LV,LV的名称设定为vbirdlv;
4. 最终这个LV格式化为ext3的文件系统，且挂载在 /mnt/lvm 中

1. #分割出4个partition,system ID为8e
[root@www ~]# fdisk /dev/hda   
...
[root@www ~]# partprobe 
[root@www ~]# fdisk -l

2. PV阶段
pvcreate: 将实体 partition 建立成为 PV;
pvscan: 搜索目前系统里面任何具有PV的磁盘;
pvdisplay: 显示出目前系统上面的PV状态;
pvremove: 将PV属性移除，让该 partition 不具有 PV 属性.

# 检查有无 PV 在系统上，然后将 /dev/hda6~/dev/hda9 建立成为 PV 格式
[root@www ~]# pvscan
No matching physical volumes found <==找不到任何的 PV 存在

#建立这四个 partition 成为 PV 
[root@www ~]# pvcreate /dev/hda{6,7,8,9}

[root@www ~]# pvscan
[root@www ~]# pvdisplay


3.VG阶段
VG相关的指令：
vgcreate: 建立VG
vgscan: 搜寻系统上面是否有VG存在
vgdisplay: 显示目前系统上面的VG状态
vgextend: 在VG内增加额外的PV
vgreduce: 在VG内移除PV
vgchange: 设定VG是否启动(active)
vgremove: 删除一个VG

vgcreate [-s N[mgt]] VG名称 PV名称
选项与参数：
-s: 后面接PE的大小(size),单位可以是m,g,t(大小写均可)

# 将 /dev/hda6-8 建立成为一个 VG，且指定 PE 为 16MB 
[root@www ~]# vgcreate -s 16M vbirdvg /dev/hda{6,7,8}
Volume group "vbirdvg" successfully created

[root@www ~]# vgscan
...
[root@www ~]# pvscan
...
[root@www ~]# vgdisplay
...

# 将剩余的 PV (/dev/hda9) 丢给 vbirdvg 
[root@www ~]# vgextend vbirdvg /dev/hda9


LV阶段
LV相关指令
lvcreate: 建立LV
lvscan: 查询系统上面的LV
lvdisplay: 显示系统上面的LV状态
lvextend: 在LV里面增加容量
lvreduce: 在LV里面减少容量
lvremove: 删除一个LV
lvresize: 对LV进行容量大小的调整

lvcreate [-L N[mgt]] [-n LV名称] VG名称
lvcreate [-l N] [-n LV名称] VG名称
选项与参数：
-L: 后面接容量，容量的单位可以是 M,G,T 等，要注意的是，最小单位为 PE，因此这个容量必项要是 PE 的倍数，若不相符，系统会自行计算最相近的容量。
-l：后面可以接 PE 的"个数"，而不是"容量"。若要这么做，得要自行计算 PE数。
-n ：后面接的就是 LV 的名称
更多的说明应该可以自行查阅 man lvcreate
# 将整个 vbirdvg 通通分配给 vbirdlv ，要注意，PE 共有 356 个。
[root@www ~]# lvcreate -l 356 -n vbirdlv vbirdvg
Logical volume "vbirdlv" created
...
[root@www ~]# ll /dev/vbirdvg/vbirdlv
lrwxrwxrwx 1 root root 27 Mar 11 16:49 /dev/vbirdvg/vbirdlv ->
/dev/mapper/vbirdvg-vbirdlv

[root@www ~]# lvdisplay
...

(注意：VG的名称为可直接使用，如vbirdvg; 但LV的名称必须使用全名，亦即是 /dev/vbirdvg/vbirdlv)


文件系统阶段
# 格式化、挂载与观察 LV 
[root@www ~]# mkfs -t ext3 /dev/vbirdvg/vbirdlv <==注意 LV 全名
[root@www ~]# mkdir /mnt/lvm
[root@www ~]# mount /dev/vbirdvg/vbirdlv /mnt/lvm
[root@www ~]# df
...

(其实 LV 的名称建置成为 /dev/vbirdvg/vbirdlv 是为了让使用者直觉式的找到我们所需要的数据， 实际上 LVM 使用的装置是放置到 /dev/mapper/ 目录下的，/dev/mapper/vbirdvg-vbirdlv)


放大LV容量
1. 用fdisk设定新的具有8e system ID 的 partition
2. 利用pvcreate建置PV
3. 利用vgextend将PV加入vbirdvg
4. 利用lvresize将新加入的PV内PE加入vbirdlv中
5. 透过resize2fs将文件系统的容量确实增加 (重要！！！针对文件系统处理)

# 1. 处理出一个新的 partition
[root@www ~]# fdisk /dev/hda 
...
[root@www ~]# partprobe
[root@www ~]# fdisk -l
...

# 2. 建立新的 PV：
[root@www ~]# pvcreate /dev/hda10
Physical volume "/dev/hda10" successfully created
[root@www ~]# pvscan
...

# 3. 加大 VG ，利用 vgextend 功能
[root@www ~]# vgextend vbirdvg /dev/hda10
...

# 4. 放大 LV 吧！利用 lvresize 的功能杢增加！
[root@www ~]# lvresize -l +179 /dev/vbirdvg/vbirdlv

[root@www ~]# lvdisplay
...
[root@www ~]# df /mnt/lvm
...

# 5.1 先看一下原本的文件系统内的 superblock 记录情况
[root@www ~]# dumpe2fs /dev/vbirdvg/vbirdlv

# 5.2 resize2fs 的语法
[root@www ~]# resize2fs [-f] [device] [size]
选项与参数：
-f ：强制进行 resize 的动作
[device]：装置的文件名；
[size] ：可以加也可以不加。如果加上 size 的话，那么就必须要给予一个单位，譬如 M, G 等等。如果没有 size 的话，那么预设使用 "整个 partition"的容量来处理

# 5.3 完整的将 LV 的容量扩充到整个 filesystem 
[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv
...
[root@www ~]# df /mnt/lvm
...


缩小LV容量
# 1. 先找出 /dev/hda6 的容量大小，并尝试计算文件系统需缩小到多少
[root@www ~]# pvdisplay
--- Physical volume ---
PV Name /dev/hda6
VG Name vbirdvg
PV Size 1.40 GB / not usable 11.46 MB
Allocatable yes (but full)
PE Size (KByte) 16384
Total PE 89
Free PE 0
Allocated PE 89
PV UUID Z13Jk5-RCls-UJ8B-HzDa-Gesn-atku-rf2biN
# 从这里可以看出 /dev/hda6 有多大，而且含有 89 个 PE 的量
# 那如果要使用 resize2fs 时，则怪量减去 1.40GB 就对了

# 2. 降低文件系统的容量
(注意：放大可以在线直接进行，缩小文件系统无法支持)
[root@www ~]# umount /mnt/lvm   #需要先卸载
[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv 6900M
resize2fs 1.39 (29-May-2006)
Please run 'e2fsck -f /dev/vbirdvg/vbirdlv' first.
# 需要先进行磁盘检查
[root@www ~]# e2fsck -f /dev/vbirdvg/vbirdlv
...
[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv 6900M
...
[root@www ~]# mount /dev/vbirdvg/vbirdlv /mnt/lvm
[root@www ~]# df /mnt/lvm
...

# 3. 降低 LV 的容量，同时我们知道 /dev/hda6 有 89 个 PE
[root@www ~]# lvresize -l -89 /dev/vbirdvg/vbirdlv
...
[root@www ~]# lvdisplay
# 4.1 先确认 /dev/hda6 是否将 PE 都移除了
[root@www ~]# pvdisplay
--- Physical volume ---
PV Name /dev/hda6
VG Name vbirdvg
PV Size 1.40 GB / not usable 11.46 MB
Allocatable yes (but full)
PE Size (KByte) 16384
Total PE 89
Free PE 0      #重点
Allocated PE 89
PV UUID Z13Jk5-RCls-UJ8B-HzDa-Gesn-atku-rf2biN
....(中间省略)....
--- Physical volume ---
PV Name /dev/hda10
VG Name vbirdvg
PV Size 2.80 GB / not usable 6.96 MB
Allocatable yes
PE Size (KByte) 16384
Total PE 179
Free PE 89     #重点
Allocated PE 90
PV UUID 7MfcG7-y9or-0Jmb-H7RO-5Pa5-D3qB-G426Vq
# 没有被使用的 PE 在 /dev/hda10，此时得要搬移 PE

[root@www ~]# pvmove /dev/hda6 /dev/hda10
# pvmove 来源 PV 目标 PV ，可以将 /dev/hda6 内的 PE 全部移动到/dev/hda10

# 4.2 将 /dev/hda6 移出 vbirdvg 中 
[root@www ~]# vgreduce vbirdvg /dev/hda6
Removed "/dev/hda6" from volume group "vbirdvg"

[root@www ~]# pvscan
...
[root@www ~]# pvremove /dev/hda6
Labels on physical volume "/dev/hda6" successfully wiped


LVM的系统快照：
将当时的系统信息记录下来，就好像照相记录一般。未来若有任何资料更动，则原始资料会被搬移到快照区，没有被更动的区域则由快照区与文件系统共享。
由于快照区与原本的LV共享很多PE区块，因此快照区与被快照的LV必须要在同一个VG中。

快照区的建立
# 1. 先观察 VG 还剩下多少剩余容量
[root@www ~]# vgdisplay
...

# 2. 将刚刚移除的 /dev/hda6 加入这个 VG
[root@www ~]# pvcreate /dev/hda6
Physical volume "/dev/hda6" successfully created
[root@www ~]# vgextend vbirdvg /dev/hda6
Volume group "vbirdvg" successfully extended
...

# 3. 利用 lvcreate 建立系统快照区，我们取名为 vbirdss，且给予 60 个 PE
[root@www ~]# lvcreate -l 60 -s -n vbirdss /dev/vbirdvg/vbirdlv
Logical volume "vbirdss" created
# 上述的指令中最重要的是那个 -s 的选项，代表是 snapshot 快照功能之意
# -n 后面接快照区的装置名称， /dev/.... 则是要被快照的 LV 完整档名。
# -l 后面则是接使用多少个 PE 来作为这个快照区使用。

[root@www ~]# lvdisplay
...

#4. 挂载快照装置
[root@www ~]# mkdir /mnt/snapshot
[root@www ~]# mount /dev/vbirdvg/vbirdss /mnt/snapshot
[root@www ~]# df
...

#5. 卸载快照装置
[root@www ~]# umount /mnt/snapshot






















