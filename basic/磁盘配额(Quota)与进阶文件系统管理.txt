磁盘配额(Quota)与进阶文件系统管理

Quota针对整个filesystem的限制项目主要分为以下几个部分：
1. 容量限制或文档数量限制(block 或 inode)
2. 柔性劝导与硬件规定(soft / hard)
3. 会倒数计时的宽限时间(grace time)


范例：Quota 的设定流程

制作账号环境
[root@www ~]# vi addaccount.sh
#!/bin/bash
#使用script来建立实验 quota 所需的环境
groupadd myquotagrp
for username in myquota1 myquota2 myquota3 myquota4 myquota5
do
	useradd -g myquotagrp $username
	echo "password" | passwd --stdin $username
done

[root@www ~]#sh addaccount.sh

流程一：文件系统支援
1.查看 /home 是否是个独立的 filesystem
[root@www ~]# df -h /home
Filesystem Size Used Avail Use% Mounted on
/dev/hda3 4.8G 740M 3.8G 17% /home <==该主机的 /home 确实是独立的
[root@www ~]# mount | grep home
/dev/hda3 on /home type ext3 (rw)

2.手动加入quota的支持
[root@www ~]#mount -o remount,usrquota,grpquota /home
[root@www ~]#mount | grep home
/dev/hda3 on /home type ext3 (rw,usrquota,grpquota)
# 重点就在于 usrquota, grpquota ！注意写法！

3.不过手动挂载的数据在下次重新挂载就会消失，因此最好写入配置文件当中
[root@www ~]# vi /etc/fstab
LABEL=/home /home ext3 defaults,usrquota,grpquota 1 2
# 其他顷目并没有列出来。重点在于第四字段。于 default 后面加上两个参数

[root@www ~]# umount /home
[root@www ~]# mount -a
[root@www ~]# mount | grep home
/dev/hda3 on /home type ext3 (rw,usrquota,grpquota)

(修改完 /etc/fstab 后，务必要测试一下。若有发生错误得要赶紧处理。因为这个文档如果修改错误，会造成无法开机完全的情况)

流程二：建立quota记录文件
quotacheck: 扫描文件系统并建立 Quota 的记录文件
quotacheck [-avugfM] [/mount_point]
选项与参数：
-a: 扫描所有在 /etc/mtab内，含有quota支持的filesystem,加上次参数后，/mount_point可不必写，因为扫描所有的filesystem
-u: 针对用户扫描文档与目录的使用情况，建立 aquota.user
-g: 针对群组扫描文档与目录的使用情况，建立 aquota.group
-v: 显示扫描过程的信息
-f: 强制扫描文件系统，并写入新的 quota 配置文件(危险)
-M: 强制以读写的方式扫描文件系统，只有在特殊情况下才会使用

# 针对整个系统含有 usrquota, grpquota 参数的文件系统进行 quotacheck 扫描
[root@www ~]# quotacheck -avug
quotacheck: Scanning /dev/hda3 [/home] quotacheck: Cannot stat old user quota
file: No such file or directory <==有找到文件系统，但尚未制作记录文件！
quotacheck: Cannot stat old group quota file: No such file or directory
quotacheck: Cannot stat old user quota file: No such file or directory
quotacheck: Cannot stat old group quota file: No such file or directory done <==上面三个错误只是说明记录文件尚未建立而已，可以忽略不理！
quotacheck: Checked 130 directories and 107 files <==实际搜寻结果
quotacheck: Old file not found.
quotacheck: Old file not found.
# 若执行这个指令却出现如下的错误讯息，表示你没有任何文件系统有启动 quota 支持！
# quotacheck: Can't find filesystem to check or filesystem not mounted with quota option.

[root@www ~]# ll -d /home/a*
-rw------- 1 root root 8192 Mar 6 11:58 /home/aquota.group
-rw------- 1 root root 9216 Mar 6 11:58 /home/aquota.user
# 在该案例中，/home 是独立的文件系统，因此搜寻结果会将两个记录文件放
在 /home 底下。这两个文档就是 Quota 最重要的信息了！

# 如果因为特殊需求需要强制扫描已挂载的文件系统时
[root@www ~]# quotacheck -avug -mf
quotacheck: Scanning /dev/hda3 [/home] done
quotacheck: Checked 130 directories and 109 files
# 资料要简洁很多。因为有记录文件存在嘛。所以警告讯息不会出现

注意：
1. 不用手动编辑记录文件，因为那两个文件时quota自动数据文件，并不是纯文本文档，且该文档会一直变动。
2. 要建立 aquota.user, aquota.group,使用 quotacheck 指令，不是手动编辑。


流程三：Quota启动、关闭与限制值设定
quotaon: 启动 quota 的服务
quotaon [-avug]
quotaon [-vug] [/mount_point]
选项与参数：
-u: 针对使用者启动 quota (aquota.user)
-g: 针对群组启动 quota (aquota.group)
-v: 显示启动过程的相关讯息
-a: 根据 /etc/mtab 内的 filesystem 设定启动有关的 quota, 若不加 -a 的话，则后面就需要加上特定的那个 filesystem 

# 由于我们要启动 user/group 的 quota ，所以使用底下的语法即可
[root@www ~]# quotaon -auvg
/dev/hda3 [/home]: group quotas turned on
/dev/hda3 [/home]: user quotas turned on

# 特殊用法，假如你的启动 /var 的 quota 支持，那么仅启动 user quota 时
[root@www ~]# quotaon -uv /var

(这个 "quotaon -auvg" 的指令几乎只在第一次启动 quota 时才需要进行。因为下次等你重新启动系统时，系统的 /etc/rc.d/rc.sysinit 这个初始化脚本就会自动的下达这个指令了)

quotaoff: 关闭 quota 的服务
quotaoff [-a]
quotaoff [-ug] [/mount_point]
选项与参数：
-a: 全部的filesystem的quota都关闭 (根据 /etc/mtab)
-u: 仅针对后面接的那个 /mount_point 关闭 user quota
-g: 仅针对后面接的那个 /mount_point 关闭 group quota

edquota: 编辑账号/群组的限值与宽限时间
edquota [-u username] [-g groupname]
edquota -t 修改宽限时间
edquota -p 范本账号 -u 新账号
选项与参数：
-u: 后面接账号名称。可以进入 quota 的编辑画面(vi)去设定 username 的限制值
-g: 后面接组名称。可以进入 quota 的编辑画面(vi)去设定 groupname 的限制值
-t: 可以修改宽限时间
-p: 复制范本。那个模板账号为已经存在并且已设定好 quota 的使用者，意义为 "将范本账号这个人的 quota 限制值复制给新账户"

设定 myquota1 这个使用者的 quota 限制值
[root@www ~]# edquota -u myquota1
Disk quotas for user myquota1 (uid 710):
Filesystem blocks soft hard inodes soft hard
/dev/hda3     80    0   0     10     0    0
设置
Disk quotas for user myquota1 (uid 710):
Filesystem blocks   soft   hard   inodes soft hard
/dev/hda3   80     250000 300000    10    0     0

# 将 myquota1 的限制值复制给其他四个账号
[root@www ~]# edquota -p myquota1 -u myquota2
[root@www ~]# edquota -p myquota1 -u myquota3
[root@www ~]# edquota -p myquota1 -u myquota4
[root@www ~]# edquota -p myquota1 -u myquota5

更改群组的quota限额
[root@www ~]# edquota -g myquotagrp
Disk quotas for group myquotagrp (gid 713):
Filesystem blocks   soft    hard  inodes soft hard
/dev/hda3    400   900000 1000000   50     0   0
# 记得，单位为 KB 

设置宽限时间
# 宽限时间原本为 7 天，将他改成 14 天吧！
[root@www ~]# edquota -t
Grace period before enforcing soft limits for users:
Time units may be: days, hours, minutes, or seconds
Filesystem    Block grace period      Inode grace period
/dev/hda3           14days                  7days
# 原本是 7days ，我们将他给改为 14days


流程四：Quota 限制值的报表
quota: 单一用户的 quota 报表
quota [-uvs] [username]
quota [-gvs] [groupname]
选项与参数：
-u: 后面可以接 username,表示显示出该用户的quota限制值。若不接 username,表示显示出执行者的 quota 限制值 
-g: 后面接 groupname, 表示显示出该群组的 quota 限制值
-v: 显示每个用户在 filesystem 的 quota 值
-s: 使用1024为倍数来指定单位，会显示如 M 之类的单位

# 直接使用 quota 去显示出 myquota1 与 myquota2 的限额
[root@www ~]# quota -uvs myquota1 myquota2
Disk quotas for user myquota1 (uid 710):
Filesystem blocks quota limit grace files quota limit grace
/dev/hda3    80    245M  293M         10    0     0
Disk quotas for user myquota2 (uid 711):
Filesystem blocks quota limit grace files quota limit grace
/dev/hda3     80   245M 293M          10    0     0
# 这个指令显示出来的数据跟 edquota 几乎是一模一样的。只是多了个 grace项目。
# 你会发现 grace 底下没有任何数据，这是因为我们的使用量 (80) 尚未超过soft

# 显示出 myquotagrp 的群组限额
[root@www ~]# quota -gvs myquotagrp
Disk quotas for group myquotagrp (gid 713):
Filesystem blocks quota limit grace files quota limit grace
/dev/hda3    400  879M  977M         50     0     0

repquota: 针对文件系统的限额做报表
repquota -a [-vugs]
选项与参数：
-a: 直接到 /etc/mtab 搜寻具有 quota 标志的 filesystem,并报告 quota 的结果
-v: 输出的数据将含有 filesystem 相关的细部信息
-u: 显示出用户的 quota 限值(这是默认值)
-g: 显示出个别群组的 quota 限值
-s: 使用M,G为单位显示结果

# 查询本案例中所有使用者的 quota 限制情况：
[root@www ~]# repquota -auvs
*** Report for user quotas on device /dev/hda3 <==针对 /dev/hda3
Block grace time: 14days; Inode grace time: 7days <==block 宽限时间为14 天
		   Block limits 			   File limits
User      used  soft  hard grace   used soft hard grace
----------------------------------------------------------------------
root --   651M   0     0             5   0    0
myquota1 -- 80  245M  293M           10  0    0
myquota2 -- 80  245M  293M           10  0    0
myquota3 -- 80  245M  293M           10  0    0
myquota4 -- 80  245M  293M           10  0    0
myquota5 -- 80  245M  293M           10  0    0
Statistics: <==这是所谓的系统相关信息，用 -v 才会显示
Total blocks: 9
Data blocks: 2
Entries: 22
Used average: 11.000000


流程5：测试与管理
# 测试1：利用 myquota1 的身份，建置一个 270MB 的大档案，幵观察 quota结果
[myquota1@www ~]# dd if=/dev/zero of=bigfile bs=1M count=270
hda3: warning, user block quota exceeded.
270+0 records in
270+0 records out
283115520 bytes (283 MB) copied, 3.20282 seconds, 88.4 MB/s

(注意：使用 myquota1 的账号去进行 dd 指令)


warnquota: 对超过限额者发出警告信(email)
[root@www ~]# warnquota
执行 warnquota 可能也不会产生任何讯息以及信件，因为只有当使用者的 quota 有超过 soft 时，warnquota 才会发送警告信。警告信的内容编辑在 /etc/warnquota.conf 中。
[root@www ~]# vi /etc/warnquota.conf
# 先找到底下这几行的设定值：
SUBJECT = NOTE: You are exceeding your allocated disk space limits <==第 10 行
CC_TO = "root@localhost" <==第 11 行
MESSAGE = Your disk usage has exceeded the agreed limits on this server|Please delete any unnecessary files on following filesystems:|   <== 第 21 行
SIGNATURE = root@localhost <==第 25 行

让系统自动执行warnquota
[root@www ~]# vi /etc/cron.daily/warnquota
/usr/sbin/warnquota
# 只要这一行，且将执行文件以绝对路径的方式写入即可


setquota: 直接于指令中设定 quota 限额
setquota [-u|-g] 名称 block(soft) block(hard) inode(soft) inode(hard) 文件系统 
范例： 
[root@www ~]# setquota -u myquota5 100000 200000 0 0 /home


不更动既有系统的quota实例：使用 ln 实现
1. 将 /var/spool/mail 这个目录完整的移动到 /home 底下；
2. 利用 ln -s /home/mail /var/spool/mail 建立链接数据；
3. 将 /home 进行 quota 限额设定


软件磁盘阵列(Software RAID)

RAID(Redundant Arrays of Inexpensive Disks,磁盘阵列)：
RAID 可以透过一个技术(软件或硬件)，将多个较小的磁盘整合成为一个较大的磁盘装置；而这个较大的磁盘功能可不止是储存而已，他还具有数据保护的功能。整个 RAID 由亍选择的等级(level)不同，而使得整合后的磁盘具有不同的功能。
常见的level类型：
1. RAID-0(等量模式,stripe): 效能最佳
2. RAID-1(映像模式,mirror): 完整备份
3. RAID 0+1, RAID 1+0
4. RAID 5：效能与数据备份的均衡考虑
5. Spare Disk: 预备磁盘的功能

磁盘阵列的优点：
1. 数据的安全与可靠性
2. 读写效能
3. 容量


软件磁盘阵列的设定
mdadm --detail /dev/md0
mdadm --create --auto=yes /dev/md[0-9] --raid-devices=N --level=[015] --spare-devices=N /dev/sdx /dev/hdx...
选项与参数：
--create: 为建立 RAID 的选项
--auto=yes: 决定建立后面接的软件磁盘阵列装置，亦即 /dev/md0,/dev/md1...
--raid-devices=N: 使用几个磁盘(parition)作为磁盘阵列的装置
--spare-devices=N: 使用几个磁盘作为备用(spare)装置
--level=[015]: 设定这组磁盘阵列的等级。支持很多，不过建议只要用0,1,5即可
--detail: 后面所接的那个磁盘阵列装置的详细信息

查看软件磁盘阵列情况：
[root@www ~]# cat /proc/mdstat

格式化与挂载使用RAID
[root@www ~]# mkfs -t ext3 /dev/md0  #/dev/md0 作为装置被格式化
[root@www ~]# mkdir /mnt/raid
[root@www ~]# mount /dev/md0 /mnt/raid
[root@www ~]# df 

仿真RAID错误的救援模式
[root@www ~]# mdadm --manage /dev/md[0-9] [-add 装置] [--remove 装置] [--fail 装置]
选项与参数：
--add: 会将后面的装置加入到这个 md 中
--remove: 会将后面的装置由这个 md 中移除
--fail: 会将后面的装置设定成为出错的状态

# 1.假设 /dev/hda8 这个装置出错了。实际模拟的方式：
[root@www ~]# mdadm --manage /dev/md0 --fail /dev/hda8
mdadm: set /dev/hda8 faulty in /dev/md0
[root@www ~]# mdadm --detail /dev/md0

将出错的磁盘移除并加入新磁盘
# 建立新的分割槽
[root@www ~]# fdisk /dev/hda
Command (m for help): n
First cylinder (2668-5005, default 2668): <==这里按 [enter]
Using default value 2668
Last cylinder or +size or +sizeM or +sizeK (2668-5005, default 5005):
+1000M
Command (m for help): w
[root@www ~]# partprobe
# 此时系统会多一个 /dev/hda11 的分割槽

# 加入新的拔除有问题的磁盘
[root@www ~]# mdadm --manage /dev/md0 --add /dev/hda11 --remove
/dev/hda8
mdadm --detail /dev/md0


开启自动启动 RAID 并自动挂载
[root@www ~]# mdadm --detail /dev/md0 | grep -i uuid
UUID : 7c60c049:57d60814:bd9a77f1:57e49c5b
# 后面那一串数据，就是这个装置向系统注册的 UUID 标识符

# 开始设定 mdadm.conf
[root@www ~]# vi /etc/mdadm.conf
ARRAY /dev/md0 UUID=7c60c049:57d60814:bd9a77f1:57e49c5b
#     RAID装置    标识符内容
# 开始设定开机自动挂载并测试
[root@www ~]# vi /etc/fstab
/dev/md0 /mnt/raid ext3 defaults 1 2
[root@www ~]# umount /dev/md0; mount -a
[root@www ~]# df /mnt/raid
Filesystem 1K-blocks Used Available Use% Mounted on
/dev/md0 2916920 188464 2580280 7% /mnt/raid
# 你得确定可以顺利挂载，并且没有发生任何错误
(如果到这里都没有出现任何问题，接下就 reboot 你的系统并等待看看能否顺利的启动)

关闭软件RAID
# 1. 先卸除且删除配置文件内与这个 /dev/md0 有关的设定：
[root@www ~]# umount /dev/md0
[root@www ~]# vi /etc/fstab
#/dev/md0 /mnt/raid ext3 defaults 1 2
# 将这一行删除掉，或者是批注掉也可以

# 2. 直接关闭 /dev/md0 的方法
[root@www ~]# mdadm --stop /dev/md0
mdadm: stopped /dev/md0 <==这样就关闭了
[root@www ~]# cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
unused devices: <none> <==确实不存在任何数组装置
[root@www ~]# vi /etc/mdadm.conf
#ARRAY /dev/md0 UUID=7c60c049:57d60814:bd9a77f1:57e49c5b
# 一样，初除他或是批注他


逻辑滚动条(Logical Volume Manager)
可以弹性的调整 filesystem 的容量。LVM可以整合多个实体partition在一起，让这些partitions看起来就像是一个磁盘一样。而且，还可以在将来新增或移除其它的实体partition 到这个 LVM 管理的磁盘当中。

PV(Physical Volume,实体滚动条)
VG(Volume Group,滚动条群组)
PE(physical Extend,实体延伸区块)，是LVM的最小储存区块
LV(Logical Volume,逻辑滚动条)

LV的装置文件名通常指定为 "/dev/vgname/lvname"
LV：写入硬盘的两种方式
1. 线性模式(linear)
2. 交错模式(triped)

LVM实作流程
(LVM必需要核心有支持且需要安装lvm2这个软件。)
1. 先分割出4个partition,每个partition的容量均为1.5G左右，且system ID 需要为 8e;
2. 全部的partition整合成为一个VG,VG的名称设定为vbirdvg;且PE的大小为16MB;
3. 全部的VG容量都丢给LV,LV的名称设定为vbirdlv;
4. 最终这个LV格式化为ext3的文件系统，且挂载在 /mnt/lvm 中

1. #分割出4个partition,system ID为8e
[root@www ~]# fdisk /dev/hda   
...
[root@www ~]# partprobe 
[root@www ~]# fdisk -l

2. PV阶段
pvcreate: 将实体 partition 建立成为 PV;
pvscan: 搜索目前系统里面任何具有PV的磁盘;
pvdisplay: 显示出目前系统上面的PV状态;
pvremove: 将PV属性移除，让该 partition 不具有 PV 属性.

# 检查有无 PV 在系统上，然后将 /dev/hda6~/dev/hda9 建立成为 PV 格式
[root@www ~]# pvscan
No matching physical volumes found <==找不到任何的 PV 存在

#建立这四个 partition 成为 PV 
[root@www ~]# pvcreate /dev/hda{6,7,8,9}

[root@www ~]# pvscan
[root@www ~]# pvdisplay


3.VG阶段
VG相关的指令：
vgcreate: 建立VG
vgscan: 搜寻系统上面是否有VG存在
vgdisplay: 显示目前系统上面的VG状态
vgextend: 在VG内增加额外的PV
vgreduce: 在VG内移除PV
vgchange: 设定VG是否启动(active)
vgremove: 删除一个VG

vgcreate [-s N[mgt]] VG名称 PV名称
选项与参数：
-s: 后面接PE的大小(size),单位可以是m,g,t(大小写均可)

# 将 /dev/hda6-8 建立成为一个 VG，且指定 PE 为 16MB 
[root@www ~]# vgcreate -s 16M vbirdvg /dev/hda{6,7,8}
Volume group "vbirdvg" successfully created

[root@www ~]# vgscan
...
[root@www ~]# pvscan
...
[root@www ~]# vgdisplay
...

# 将剩余的 PV (/dev/hda9) 丢给 vbirdvg 
[root@www ~]# vgextend vbirdvg /dev/hda9


LV阶段
LV相关指令
lvcreate: 建立LV
lvscan: 查询系统上面的LV
lvdisplay: 显示系统上面的LV状态
lvextend: 在LV里面增加容量
lvreduce: 在LV里面减少容量
lvremove: 删除一个LV
lvresize: 对LV进行容量大小的调整

lvcreate [-L N[mgt]] [-n LV名称] VG名称
lvcreate [-l N] [-n LV名称] VG名称
选项与参数：
-L: 后面接容量，容量的单位可以是 M,G,T 等，要注意的是，最小单位为 PE，因此这个容量必项要是 PE 的倍数，若不相符，系统会自行计算最相近的容量。
-l：后面可以接 PE 的"个数"，而不是"容量"。若要这么做，得要自行计算 PE数。
-n ：后面接的就是 LV 的名称
更多的说明应该可以自行查阅 man lvcreate
# 将整个 vbirdvg 通通分配给 vbirdlv ，要注意，PE 共有 356 个。
[root@www ~]# lvcreate -l 356 -n vbirdlv vbirdvg
Logical volume "vbirdlv" created
...
[root@www ~]# ll /dev/vbirdvg/vbirdlv
lrwxrwxrwx 1 root root 27 Mar 11 16:49 /dev/vbirdvg/vbirdlv ->
/dev/mapper/vbirdvg-vbirdlv

[root@www ~]# lvdisplay
...

(注意：VG的名称为可直接使用，如vbirdvg; 但LV的名称必须使用全名，亦即是 /dev/vbirdvg/vbirdlv)


文件系统阶段
# 格式化、挂载与观察 LV 
[root@www ~]# mkfs -t ext3 /dev/vbirdvg/vbirdlv <==注意 LV 全名
[root@www ~]# mkdir /mnt/lvm
[root@www ~]# mount /dev/vbirdvg/vbirdlv /mnt/lvm
[root@www ~]# df
...

(其实 LV 的名称建置成为 /dev/vbirdvg/vbirdlv 是为了让使用者直觉式的找到我们所需要的数据， 实际上 LVM 使用的装置是放置到 /dev/mapper/ 目录下的，/dev/mapper/vbirdvg-vbirdlv)


放大LV容量
1. 用fdisk设定新的具有8e system ID 的 partition
2. 利用pvcreate建置PV
3. 利用vgextend将PV加入vbirdvg
4. 利用lvresize将新加入的PV内PE加入vbirdlv中
5. 透过resize2fs将文件系统的容量确实增加 (重要！！！针对文件系统处理)

# 1. 处理出一个新的 partition
[root@www ~]# fdisk /dev/hda 
...
[root@www ~]# partprobe
[root@www ~]# fdisk -l
...

# 2. 建立新的 PV：
[root@www ~]# pvcreate /dev/hda10
Physical volume "/dev/hda10" successfully created
[root@www ~]# pvscan
...

# 3. 加大 VG ，利用 vgextend 功能
[root@www ~]# vgextend vbirdvg /dev/hda10
...

# 4. 放大 LV 吧！利用 lvresize 的功能杢增加！
[root@www ~]# lvresize -l +179 /dev/vbirdvg/vbirdlv

[root@www ~]# lvdisplay
...
[root@www ~]# df /mnt/lvm
...

# 5.1 先看一下原本的文件系统内的 superblock 记录情况
[root@www ~]# dumpe2fs /dev/vbirdvg/vbirdlv

# 5.2 resize2fs 的语法
[root@www ~]# resize2fs [-f] [device] [size]
选项与参数：
-f ：强制进行 resize 的动作
[device]：装置的文件名；
[size] ：可以加也可以不加。如果加上 size 的话，那么就必须要给予一个单位，譬如 M, G 等等。如果没有 size 的话，那么预设使用 "整个 partition"的容量来处理

# 5.3 完整的将 LV 的容量扩充到整个 filesystem 
[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv
...
[root@www ~]# df /mnt/lvm
...


缩小LV容量
# 1. 先找出 /dev/hda6 的容量大小，并尝试计算文件系统需缩小到多少
[root@www ~]# pvdisplay
--- Physical volume ---
PV Name /dev/hda6
VG Name vbirdvg
PV Size 1.40 GB / not usable 11.46 MB
Allocatable yes (but full)
PE Size (KByte) 16384
Total PE 89
Free PE 0
Allocated PE 89
PV UUID Z13Jk5-RCls-UJ8B-HzDa-Gesn-atku-rf2biN
# 从这里可以看出 /dev/hda6 有多大，而且含有 89 个 PE 的量
# 那如果要使用 resize2fs 时，则怪量减去 1.40GB 就对了

# 2. 降低文件系统的容量
(注意：放大可以在线直接进行，缩小文件系统无法支持)
[root@www ~]# umount /mnt/lvm   #需要先卸载
[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv 6900M
resize2fs 1.39 (29-May-2006)
Please run 'e2fsck -f /dev/vbirdvg/vbirdlv' first.
# 需要先进行磁盘检查
[root@www ~]# e2fsck -f /dev/vbirdvg/vbirdlv
...
[root@www ~]# resize2fs /dev/vbirdvg/vbirdlv 6900M
...
[root@www ~]# mount /dev/vbirdvg/vbirdlv /mnt/lvm
[root@www ~]# df /mnt/lvm
...

# 3. 降低 LV 的容量，同时我们知道 /dev/hda6 有 89 个 PE
[root@www ~]# lvresize -l -89 /dev/vbirdvg/vbirdlv
...
[root@www ~]# lvdisplay
# 4.1 先确认 /dev/hda6 是否将 PE 都移除了
[root@www ~]# pvdisplay
--- Physical volume ---
PV Name /dev/hda6
VG Name vbirdvg
PV Size 1.40 GB / not usable 11.46 MB
Allocatable yes (but full)
PE Size (KByte) 16384
Total PE 89
Free PE 0      #重点
Allocated PE 89
PV UUID Z13Jk5-RCls-UJ8B-HzDa-Gesn-atku-rf2biN
....(中间省略)....
--- Physical volume ---
PV Name /dev/hda10
VG Name vbirdvg
PV Size 2.80 GB / not usable 6.96 MB
Allocatable yes
PE Size (KByte) 16384
Total PE 179
Free PE 89     #重点
Allocated PE 90
PV UUID 7MfcG7-y9or-0Jmb-H7RO-5Pa5-D3qB-G426Vq
# 没有被使用的 PE 在 /dev/hda10，此时得要搬移 PE

[root@www ~]# pvmove /dev/hda6 /dev/hda10
# pvmove 来源 PV 目标 PV ，可以将 /dev/hda6 内的 PE 全部移动到/dev/hda10

# 4.2 将 /dev/hda6 移出 vbirdvg 中 
[root@www ~]# vgreduce vbirdvg /dev/hda6
Removed "/dev/hda6" from volume group "vbirdvg"

[root@www ~]# pvscan
...
[root@www ~]# pvremove /dev/hda6
Labels on physical volume "/dev/hda6" successfully wiped


LVM的系统快照：
将当时的系统信息记录下来，就好像照相记录一般。未来若有任何资料更动，则原始资料会被搬移到快照区，没有被更动的区域则由快照区与文件系统共享。
由于快照区与原本的LV共享很多PE区块，因此快照区与被快照的LV必须要在同一个VG中。

快照区的建立
# 1. 先观察 VG 还剩下多少剩余容量
[root@www ~]# vgdisplay
...

# 2. 将刚刚移除的 /dev/hda6 加入这个 VG
[root@www ~]# pvcreate /dev/hda6
Physical volume "/dev/hda6" successfully created
[root@www ~]# vgextend vbirdvg /dev/hda6
Volume group "vbirdvg" successfully extended
...

# 3. 利用 lvcreate 建立系统快照区，我们取名为 vbirdss，且给予 60 个 PE
[root@www ~]# lvcreate -l 60 -s -n vbirdss /dev/vbirdvg/vbirdlv
Logical volume "vbirdss" created  #打印信息，不是指令
# 上述的指令中最重要的是那个 -s 的选项，代表是 snapshot 快照功能之意
# -n 后面接快照区的装置名称， /dev/.... 则是要被快照的 LV 完整档名。
# -l 后面则是接使用多少个 PE 来作为这个快照区使用。

[root@www ~]# lvdisplay
...

#4. 挂载快照装置
[root@www ~]# mkdir /mnt/snapshot
[root@www ~]# mount /dev/vbirdvg/vbirdss /mnt/snapshot
[root@www ~]# df
...

#5. 卸载快照装置
[root@www ~]# umount /mnt/snapshot


范例：利用快照区复原系统
# 1. 先将原本的 /dev/vbirdvg/vbirdlv 内容作些变更，增增减减一些目弽吧！
[root@www ~]# df /mnt/lvm
...
[root@www ~]# ll /mnt/lvm
...
[root@www ~]# rm -r /mnt/lvm/log
...
[root@www ~]# ll /mnt/lvm
...
[root@www ~]# lvdisplay /dev/vbirdvg/vbirdss
...
Allocated to snapshot 12.22%
...
# 从这里可以看出，忚照区已经被使用了 12.22%。因为原始的文件系统有改动过。

# 2. 利用快照区将原本的 filesystem 备份
[root@www ~]# mount /dev/vbirdvg/vbirdss /mnt/snapshot
[root@www ~]# df
...

[root@www ~]# mkdir -p /backups <==确认真的有这个目录
[root@www ~]# cd /mnt/snapshot
[root@www snapshot]# tar -jcv -f /backups/lvm.tar.bz2 *
# 此时你就会有一个备份资料，亦即是 /backups/lvm.tar.bz2

# 3. 将 vbirdss 卸除并移除 (因为里面的内容已经备份起来了)
[root@www ~]# umount /mnt/snapshot
[root@www ~]# lvremove /dev/vbirdvg/vbirdss
...
[root@www ~]# umount /mnt/lvm
[root@www ~]# mkfs -t ext3 /dev/vbirdvg/vbirdlv
[root@www ~]# mount /dev/vbirdvg/vbirdlv /mnt/lvm
[root@www ~]# tar -jxv -f /backups/lvm.tar.bz2 -C /mnt/lvm
[root@www ~]# ll /mnt/lvm


范例：利用快照区进行各项练习与测试的任务，再以原系统还原快照
#1.建立一个大一些的快照区，将 /dev/hda6 的PE全部给快照区
[root@www ~]# lvcreate -s -l 89 -n vbirds /dev/vbirdvg/vibrdlv
...
[root@www ~]# lvdisplay /dev/vbirdvg/vbirdss
...

#2.隐藏vbirdlv 挂载vbirdss
[root@www ~]# umount /mnt/lvm
[root@www ~]# mount /dev/vbirdvg/vbirdss /mnt/snapshot
[root@www ~]# df /mnt/snapshot
...

# 3.做一些改动
[root@www ~]# rm -r /mnt/snapshot/etc /mnt/snapshot/log
[root@www ~]# cp -a /boot /lib /sbin /mnt/snapshot/
[root@www ~]# ll /mnt/snapshot

[root@www ~]# mount /dev/vbirdvg/vbirdlv /mnt/lvm
[root@www ~]# ll /mnt/lvm
...
#不论在快照区如何做改动，原本的vbirdlv里面的数据不变

#4还原原本快照区的数据，回到与原文件系统相同的信息
[root@www ~]# umount /mnt/snapshot
[root@www ~]# lvremove /dev/vbirdvg/vbirdss
...
[root@www ~]# lvcreate -s -l 89 -n vbirdss /dev/vbirdvg/vbirdlv
[root@www ~]# mount /dev/vbirdvg/vbirdss /mnt/snapshot
[root@www ~]# ll mnt/snapshot
...
#数据还原

LVM指令汇总
任务		    	PV阶段 		VG阶段		LV 阶段
搜寻(scan) 			pvscan 		vgscan 		lvscan
建立(create) 		pvcreate 	vgcreate 	lvcreate
列出(display) 		pvdisplay 	vgdisplay 	lvdisplay
增加(extend) 					vgextend 	lvextend(lvresize)
减少(reduce) 					vgreduce 	lvreduce(lvresize)
初除(remove) 		pvremove 	vgremove 	lvremove
改变容量(resize) 							lvresize
改变属性(attribute) pvchange 	vgchange 		lvchange



将LVM的装置关闭并移除
1.先卸除系统上面的LVM文件系统(包括快照与所有LV);
2.使用lvremove移除LV;
3.使用vgchange -a n VGname 让 VGname 这个 VG 不具有Active的标志;
4.使用vgremove移除VG;
5.使用pvremove移除PV;
6.最后，使用fdisk修改ID回来。

[root@www ~]# umount /mnt/lvm
[root@www ~]# umount /mnt/snapshot
[root@www ~]# lvremove /dev/vbirdvg/vbirdss  #先处理快照
...
[root@www ~]# lvremove /dev/vbirdvg/vbirdlv  #再处理原系统
...
[root@www ~]# vgchange -a n vbirdvg
...
[root@www ~]# pvremove /dev/hda(6,7,8,9,10)
...
最后再fdisk将磁盘ID改回来














